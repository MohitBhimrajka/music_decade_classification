{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "from torch_lr_finder import LRFinder\n",
    "import importlib\n",
    "import inspect\n",
    "\n",
    "# --- Add src to path ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# --- Import necessary modules ---\n",
    "try:\n",
    "    from utils import load_processed_data\n",
    "    from models import Model_1, Model_2, Model_3, ACTIVATION_FUNCTIONS # Assuming revised models.py\n",
    "except ImportError as e:\n",
    "    print(f\"Initial import failed: {e}. Ensure src is in path and files exist.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 128\n",
    "INITIAL_MODEL_CLASS = Model_2 # From Part 2\n",
    "SEED = 42\n",
    "PLOT_SAVE_DIR = \"../results/plots/\"\n",
    "N_EPOCHS_VERIFY = 5\n",
    "K_FOLDS = 5\n",
    "N_EPOCHS_KFOLD = 10\n",
    "WEIGHT_DECAY_VALUES = [0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "N_EPOCHS_COMPONENT_TEST = 15\n",
    "\n",
    "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Seed ---\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Reproducibility seed set to: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "print(\"\\nLoading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_processed_data()\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Evaluation Function ---\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "    if total_samples == 0: return 0.0, 0.0\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function for component training loop ---\n",
    "def run_component_training(model, optimizer_class, criterion, train_loader, val_loader,\n",
    "                           lr, wd, epochs, device, model_name=\"Model\"):\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    print(f\"Starting training: {model_name} - {epochs} epochs, LR={lr}, WD={wd}, Optim={optimizer_class.__name__}\")\n",
    "    train_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        epoch_val_loss, epoch_val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        epoch_end_time = time.time()\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or (epoch + 1 == epochs) :\n",
    "            print(f\"  Epoch {epoch+1}/{epochs} -> Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f} ({(epoch_end_time - epoch_start_time):.2f}s)\")\n",
    "    train_end_time = time.time()\n",
    "    print(f\"Finished training {model_name}. Total time: {(train_end_time - train_start_time):.2f}s\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*30 + \" Phase 4: Learning Rate Optimization \" + \"=\"*30)\n",
    "\n",
    "# --- LR Finder Setup ---\n",
    "print(\"\\n--- Setting up Learning Rate Finder ---\")\n",
    "model_lr = INITIAL_MODEL_CLASS().to(DEVICE)\n",
    "optimizer_lr = optim.Adam(model_lr.parameters(), lr=1e-7) # Start low\n",
    "criterion_lr = nn.CrossEntropyLoss()\n",
    "lr_finder = LRFinder(model_lr, optimizer_lr, criterion_lr, device=DEVICE)\n",
    "num_iterations_lr = len(train_loader)\n",
    "print(f\"LR Finder will run for approximately {num_iterations_lr} iterations (1 epoch).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run LR Finder ---\n",
    "print(\"Running LR Finder...\")\n",
    "start_time = time.time()\n",
    "# CORRECTED CALL: Removed skip_start, skip_end\n",
    "lr_finder.range_test(train_loader, end_lr=1, num_iter=num_iterations_lr, step_mode=\"exp\")\n",
    "end_time = time.time()\n",
    "print(f\"LR Finder finished in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot LR Finder Results ---\n",
    "print(\"\\nPlotting LR vs. Loss...\")\n",
    "lr_finder_fig_path = os.path.join(PLOT_SAVE_DIR, 'lr_finder_plot.png')\n",
    "# CORRECTED CALL: Removed unsupported args, plotting to current figure\n",
    "fig, ax = plt.subplots() # Create figure manually\n",
    "lr_finder.plot(ax=ax, log_lr=True) # Plot to the created axes\n",
    "fig.suptitle(\"Learning Rate Finder Results\", y=1.02)\n",
    "fig.savefig(lr_finder_fig_path) # Save manually\n",
    "print(f\"LR Finder plot saved to {lr_finder_fig_path}\")\n",
    "plt.show() # Show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze the Plot and Select LR ---\n",
    "print(\"\\n--- Analysis ---\")\n",
    "print(\"Examine the generated plot ('lr_finder_plot.png').\")\n",
    "# **MANUALLY ADJUST THIS BASED ON YOUR PLOT**\n",
    "suggested_lr_from_finder = 1e-3\n",
    "print(f\"Suggested LR based on visual inspection (ADJUST IF NEEDED): {suggested_lr_from_finder}\")\n",
    "\n",
    "lr_finder.reset()\n",
    "print(\"LR Finder state and model weights have been reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification ---\n",
    "print(\"\\n--- Verifying Suggested LR ---\")\n",
    "model_verify = INITIAL_MODEL_CLASS().to(DEVICE)\n",
    "# Note: Using Adam here consistent with LR finder optimizer\n",
    "optimizer_verify = optim.Adam(model_verify.parameters(), lr=suggested_lr_from_finder)\n",
    "criterion_verify = nn.CrossEntropyLoss()\n",
    "\n",
    "verification_history = run_component_training(\n",
    "    model=model_verify, optimizer_class=optim.Adam, criterion=criterion_verify,\n",
    "    train_loader=train_loader, val_loader=val_loader, lr=suggested_lr_from_finder, wd=0,\n",
    "    epochs=N_EPOCHS_VERIFY, device=DEVICE, model_name=f\"LR_Verify_{suggested_lr_from_finder}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot verification results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1); plt.plot(range(1, N_EPOCHS_VERIFY + 1), verification_history['train_loss'], label='Training Loss'); plt.plot(range(1, N_EPOCHS_VERIFY + 1), verification_history['val_loss'], label='Validation Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Loss (LR={suggested_lr_from_finder})'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2); plt.plot(range(1, N_EPOCHS_VERIFY + 1), verification_history['val_acc'], label='Validation Accuracy', color='green'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title(f'Accuracy (LR={suggested_lr_from_finder})'); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout(); plt.savefig(os.path.join(PLOT_SAVE_DIR, f'lr_{suggested_lr_from_finder}_verification.png')); plt.show()\n",
    "\n",
    "print(\"\\nVerification Complete. Ensure learning is stable.\")\n",
    "OPTIMAL_LR = suggested_lr_from_finder\n",
    "print(f\"Set OPTIMAL_LR = {OPTIMAL_LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*50); print(\"--- Phase 5: Advanced Optimization Techniques ---\"); print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 5.1 Weight Decay Optimization (using K-Fold CV) ---\n",
    "print(\"\\n--- 5.1 Weight Decay (L2 Regularization) Optimization ---\")\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "kfold_results = {wd: [] for wd in WEIGHT_DECAY_VALUES}\n",
    "kfold_val_losses = {wd: [] for wd in WEIGHT_DECAY_VALUES}\n",
    "print(f\"Starting K-Fold CV (k={K_FOLDS}) for WD values: {WEIGHT_DECAY_VALUES}\"); print(f\"Training each fold for {N_EPOCHS_KFOLD} epochs with LR={OPTIMAL_LR}\")\n",
    "fold_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wd_value in WEIGHT_DECAY_VALUES:\n",
    "    print(f\"\\n-- Testing Weight Decay = {wd_value} --\")\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        fold_loop_start_time = time.time()\n",
    "        X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train[val_idx], y_train[val_idx]\n",
    "        train_fold_dataset = TensorDataset(X_train_fold, y_train_fold); val_fold_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_fold_loader = DataLoader(train_fold_dataset, batch_size=BATCH_SIZE, shuffle=True); val_fold_loader = DataLoader(val_fold_dataset, batch_size=BATCH_SIZE * 2)\n",
    "        model_fold = INITIAL_MODEL_CLASS().to(DEVICE)\n",
    "        optimizer_fold = optim.Adam(model_fold.parameters(), lr=OPTIMAL_LR, weight_decay=wd_value); criterion_fold = nn.CrossEntropyLoss()\n",
    "        for epoch in range(N_EPOCHS_KFOLD):\n",
    "            model_fold.train()\n",
    "            for batch in train_fold_loader:\n",
    "                inputs, targets = batch; inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                optimizer_fold.zero_grad(); outputs = model_fold(inputs); loss = criterion_fold(outputs, targets)\n",
    "                loss.backward(); optimizer_fold.step()\n",
    "        final_val_loss, final_val_acc = evaluate(model_fold, val_fold_loader, criterion_fold, DEVICE)\n",
    "        fold_accuracies.append(final_val_acc); fold_losses.append(final_val_loss)\n",
    "        fold_loop_end_time = time.time()\n",
    "        print(f\"    Fold {fold+1} finished. Val Loss: {final_val_loss:.4f}, Val Acc: {final_val_acc:.4f} (Time: {fold_loop_end_time - fold_loop_start_time:.2f}s)\")\n",
    "    kfold_results[wd_value] = fold_accuracies; kfold_val_losses[wd_value] = fold_losses\n",
    "    print(f\"  Finished testing WD = {wd_value}. Avg Acc: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "fold_end_time = time.time(); print(f\"\\nK-Fold CV finished in {(fold_end_time - fold_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze K-Fold Results ---\n",
    "print(\"\\n--- Weight Decay K-Fold CV Results Summary ---\")\n",
    "avg_accuracies = {wd: np.mean(accs) for wd, accs in kfold_results.items()}; std_accuracies = {wd: np.std(accs) for wd, accs in kfold_results.items()}\n",
    "avg_losses = {wd: np.mean(losses) for wd, losses in kfold_val_losses.items()}\n",
    "print(f\"{'Weight Decay':<15} | {'Avg Val Acc':<15} | {'Std Val Acc':<15} | {'Avg Val Loss':<15}\"); print(\"-\" * 65)\n",
    "for wd_value in WEIGHT_DECAY_VALUES: print(f\"{wd_value:<15} | {avg_accuracies[wd_value]:.4f}{' ':<10} | {std_accuracies[wd_value]:.4f}{' ':<10} | {avg_losses[wd_value]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot K-Fold Results ---\n",
    "wd_values_str = [str(wd) for wd in WEIGHT_DECAY_VALUES]; avg_acc_list = [avg_accuracies[wd] for wd in WEIGHT_DECAY_VALUES]; std_acc_list = [std_accuracies[wd] for wd in WEIGHT_DECAY_VALUES]\n",
    "plt.figure(figsize=(10, 6)); plt.errorbar(wd_values_str, avg_acc_list, yerr=std_acc_list, marker='o', capsize=5); plt.xlabel('Weight Decay Value'); plt.ylabel('Average Validation Accuracy')\n",
    "plt.title(f'Weight Decay Optimization ({K_FOLDS}-Fold CV, {N_EPOCHS_KFOLD} Epochs/Fold)'); plt.grid(True, linestyle='--', alpha=0.6); plt.ylim(min(avg_acc_list) - 0.005, max(avg_acc_list) + 0.005)\n",
    "plt.tight_layout(); plt.savefig(os.path.join(PLOT_SAVE_DIR, 'weight_decay_kfold_cv.png')); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select Best Weight Decay ---\n",
    "OPTIMAL_WEIGHT_DECAY = max(avg_accuracies, key=avg_accuracies.get)\n",
    "print(f\"\\nBest Weight Decay (based on K-Fold): {OPTIMAL_WEIGHT_DECAY} (Avg Acc: {avg_accuracies[OPTIMAL_WEIGHT_DECAY]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 Neural Network Component Optimization ---\n",
    "print(\"\\n\\n\" + \"=\"*50); print(\"--- 5.2 Neural Network Component Optimization ---\"); print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Baseline Configuration ---\n",
    "BASELINE_LR = OPTIMAL_LR; BASELINE_WD = OPTIMAL_WEIGHT_DECAY; BASELINE_MODEL_CLASS = INITIAL_MODEL_CLASS; BASELINE_OPTIMIZER = optim.Adam\n",
    "print(f\"Baseline Config: LR={BASELINE_LR}, WD={BASELINE_WD}, Model={BASELINE_MODEL_CLASS.__name__}, Optimizer={BASELINE_OPTIMIZER.__name__}\"); print(f\"Training each component test for {N_EPOCHS_COMPONENT_TEST} epochs.\")\n",
    "component_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2.1 Weight Initialization ---\n",
    "print(\"\\n--- 5.2.1 Testing Weight Initialization ---\")\n",
    "initialization_strategies = {\"Default (Kaiming Uniform for ReLU)\": None, \"Xavier Uniform\": nn.init.xavier_uniform_, \"Kaiming Normal\": nn.init.kaiming_normal_}\n",
    "initialization_results = {}\n",
    "for init_name, init_func in initialization_strategies.items():\n",
    "    print(f\"\\n-- Testing Initialization: {init_name} --\")\n",
    "    model_init = BASELINE_MODEL_CLASS().to(DEVICE) # Using default activation (ReLU)\n",
    "    criterion_init = nn.CrossEntropyLoss()\n",
    "    if init_func is not None:\n",
    "        def initialize_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                try: gain = nn.init.calculate_gain('relu') if 'kaiming' in init_name.lower() else 1.0; init_func(m.weight, gain=gain)\n",
    "                except TypeError: init_func(m.weight)\n",
    "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "        model_init.apply(initialize_weights); print(\"Applied custom weight initialization.\")\n",
    "    else: print(\"Using default PyTorch weight initialization.\")\n",
    "    history = run_component_training(model=model_init, optimizer_class=BASELINE_OPTIMIZER, criterion=criterion_init, train_loader=train_loader, val_loader=val_loader, lr=BASELINE_LR, wd=BASELINE_WD, epochs=N_EPOCHS_COMPONENT_TEST, device=DEVICE, model_name=f\"Init_{init_name}\")\n",
    "    initialization_results[init_name] = history; component_results[f\"Init_{init_name}\"] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze Initialization Results ---\n",
    "print(\"\\n--- Weight Initialization Results Summary ---\")\n",
    "print(f\"{'Initialization':<35} | {'Final Val Loss':<15} | {'Final Val Acc':<15} | {'Max Val Acc':<15}\"); print(\"-\" * 85)\n",
    "best_init_name = \"\"; best_init_max_acc = -1.0\n",
    "for name, history in initialization_results.items():\n",
    "    final_val_loss = history['val_loss'][-1]; final_val_acc = history['val_acc'][-1]; max_val_acc = max(history['val_acc'])\n",
    "    print(f\"{name:<35} | {final_val_loss:<15.4f} | {final_val_acc:<15.4f} | {max_val_acc:<15.4f}\")\n",
    "    if max_val_acc > best_init_max_acc: best_init_max_acc = max_val_acc; best_init_name = name\n",
    "print(f\"\\nBest Initialization Strategy (Max Val Acc): {best_init_name} ({best_init_max_acc:.4f})\")\n",
    "# Sticking with default unless a clear winner emerges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Initialization comparison ---\n",
    "plt.figure(figsize=(12, 6));\n",
    "for name, history in initialization_results.items(): plt.plot(range(1, N_EPOCHS_COMPONENT_TEST + 1), history['val_acc'], label=name, marker='.', linestyle='--')\n",
    "plt.title(f'Validation Accuracy Comparison: Initialization Strategies ({N_EPOCHS_COMPONENT_TEST} Epochs)'); plt.xlabel('Epoch'); plt.ylabel('Validation Accuracy'); plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'component_initialization_comparison.png')); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30 + \" Reloading Models Module \" + \"=\"*30)\n",
    "if 'models' in sys.modules:\n",
    "    print(\"Attempting to reload 'models' module...\"); import models; importlib.reload(models); from models import Model_1, Model_2, Model_3, ACTIVATION_FUNCTIONS\n",
    "    print(\"'models' module reloaded.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))\n",
    "    BASELINE_MODEL_CLASS = Model_2 # Update reference\n",
    "else:\n",
    "    print(\"Importing 'models' module...\"); import models; from models import Model_1, Model_2, Model_3, ACTIVATION_FUNCTIONS\n",
    "    BASELINE_MODEL_CLASS = Model_2\n",
    "    print(\"'models' module imported.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5.2.2 Testing Activation Functions ---\")\n",
    "activation_functions_to_test = {\"ReLU\": nn.ReLU, \"LeakyReLU\": nn.LeakyReLU, \"GELU\": nn.GELU}\n",
    "activation_results = {}\n",
    "Activation_Model_Class = BASELINE_MODEL_CLASS # Use reloaded Model_2\n",
    "\n",
    "for act_name, act_fn_class in activation_functions_to_test.items():\n",
    "    print(f\"\\n-- Testing Activation: {act_name} --\")\n",
    "    model_act = Activation_Model_Class(activation_fn=act_fn_class, activation_name=act_name).to(DEVICE) # Pass class and name\n",
    "    criterion_act = nn.CrossEntropyLoss()\n",
    "    print(\"Using default PyTorch weight initialization.\"); print(model_act)\n",
    "    history = run_component_training(model=model_act, optimizer_class=BASELINE_OPTIMIZER, criterion=criterion_act, train_loader=train_loader, val_loader=val_loader, lr=BASELINE_LR, wd=BASELINE_WD, epochs=N_EPOCHS_COMPONENT_TEST, device=DEVICE, model_name=f\"Activation_{act_name}\")\n",
    "    activation_results[act_name] = history; component_results[f\"Activation_{act_name}\"] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Activation Function Results Summary ---\")\n",
    "print(f\"{'Activation':<15} | {'Final Val Loss':<15} | {'Final Val Acc':<15} | {'Max Val Acc':<15}\"); print(\"-\" * 65)\n",
    "best_act_name = \"\"; best_act_max_acc = -1.0\n",
    "for name, history in activation_results.items():\n",
    "    final_val_loss = history['val_loss'][-1]; final_val_acc = history['val_acc'][-1]; max_val_acc = max(history['val_acc'])\n",
    "    print(f\"{name:<15} | {final_val_loss:<15.4f} | {final_val_acc:<15.4f} | {max_val_acc:<15.4f}\")\n",
    "    if max_val_acc > best_act_max_acc: best_act_max_acc = max_val_acc; best_act_name = name\n",
    "print(f\"\\nBest Activation Function (Max Val Acc): {best_act_name} ({best_act_max_acc:.4f})\")\n",
    "if best_act_name in activation_functions_to_test: OPTIMAL_ACTIVATION_FN = activation_functions_to_test[best_act_name]; print(f\"Stored OPTIMAL_ACTIVATION_FN: {OPTIMAL_ACTIVATION_FN}\")\n",
    "else: print(\"Error: Best activation name not found!\"); OPTIMAL_ACTIVATION_FN = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Activation comparison ---\n",
    "plt.figure(figsize=(12, 6));\n",
    "for name, history in activation_results.items(): plt.plot(range(1, N_EPOCHS_COMPONENT_TEST + 1), history['val_acc'], label=name, marker='.', linestyle='--')\n",
    "plt.title(f'Validation Accuracy Comparison: Activation Functions ({N_EPOCHS_COMPONENT_TEST} Epochs)'); plt.xlabel('Epoch'); plt.ylabel('Validation Accuracy'); plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'component_activation_comparison.png')); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNext steps: Test Normalization Layers and Optimizers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# == Module Reloading (Ensure latest models.py for normalization tests) ==\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*30 + \" Reloading Models Module (Before Norm Tests) \" + \"=\"*30)\n",
    "if 'models' in sys.modules:\n",
    "    print(\"Attempting to reload 'models' module...\"); import models; importlib.reload(models); from models import Model_1, Model_2, Model_3, ACTIVATION_FUNCTIONS\n",
    "    print(\"'models' module reloaded.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))\n",
    "    BASELINE_MODEL_CLASS = Model_2 # Update reference to potentially new Model_2\n",
    "else:\n",
    "    print(\"Importing 'models' module...\"); import models; from models import Model_1, Model_2, Model_3, ACTIVATION_FUNCTIONS\n",
    "    BASELINE_MODEL_CLASS = Model_2\n",
    "    print(\"'models' module imported.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# == Normalization Layer Test ==\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5.2.3 Testing Normalization Layers ---\")\n",
    "\n",
    "# Define normalization strategies to test\n",
    "# Use string identifiers that match get_norm_layer function\n",
    "normalization_strategies_to_test = {\n",
    "    \"None\": None,\n",
    "    \"BatchNorm\": \"batch\",\n",
    "    \"LayerNorm\": \"layer\",\n",
    "}\n",
    "\n",
    "normalization_results = {}\n",
    "Normalization_Model_Class = BASELINE_MODEL_CLASS # Should be reloaded Model_2\n",
    "Best_Activation_Class = OPTIMAL_ACTIVATION_FN   # Use GELU determined previously\n",
    "Best_Activation_Name = best_act_name            # 'GELU'\n",
    "\n",
    "for norm_name, norm_type_str in normalization_strategies_to_test.items():\n",
    "    print(f\"\\n-- Testing Normalization: {norm_name} --\")\n",
    "\n",
    "    # Instantiate model with the specific normalization type and best activation\n",
    "    model_norm = Normalization_Model_Class(\n",
    "        activation_fn=Best_Activation_Class,\n",
    "        activation_name=Best_Activation_Name,\n",
    "        norm_layer_type=norm_type_str # Pass the string identifier or None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion_norm = nn.CrossEntropyLoss()\n",
    "    print(f\"Using default initialization, Activation={Best_Activation_Name}, Norm={norm_name}\")\n",
    "    print(model_norm) # Print model info\n",
    "\n",
    "    # Train the model\n",
    "    history = run_component_training(\n",
    "        model=model_norm,\n",
    "        optimizer_class=BASELINE_OPTIMIZER, # Adam\n",
    "        criterion=criterion_norm,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        lr=BASELINE_LR,\n",
    "        wd=BASELINE_WD,\n",
    "        epochs=N_EPOCHS_COMPONENT_TEST, # Use same number of epochs\n",
    "        device=DEVICE,\n",
    "        model_name=f\"Norm_{norm_name}\"\n",
    "    )\n",
    "    normalization_results[norm_name] = history\n",
    "    component_results[f\"Norm_{norm_name}\"] = history # Store in main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze Normalization Layer Results ---\n",
    "print(\"\\n--- Normalization Layer Results Summary ---\")\n",
    "print(f\"{'Normalization':<15} | {'Final Val Loss':<15} | {'Final Val Acc':<15} | {'Max Val Acc':<15}\")\n",
    "print(\"-\" * 65)\n",
    "best_norm_name = \"\"\n",
    "best_norm_max_acc = -1.0\n",
    "\n",
    "for name, history in normalization_results.items():\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    max_val_acc = max(history['val_acc'])\n",
    "    print(f\"{name:<15} | {final_val_loss:<15.4f} | {final_val_acc:<15.4f} | {max_val_acc:<15.4f}\")\n",
    "    if max_val_acc > best_norm_max_acc:\n",
    "        best_norm_max_acc = max_val_acc\n",
    "        best_norm_name = name\n",
    "\n",
    "print(f\"\\nBest Normalization Strategy (based on Max Validation Accuracy): {best_norm_name} ({best_norm_max_acc:.4f})\")\n",
    "OPTIMAL_NORM_TYPE = normalization_strategies_to_test[best_norm_name] # Store the type ('batch', 'layer', or None)\n",
    "print(f\"Stored OPTIMAL_NORM_TYPE: {OPTIMAL_NORM_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Normalization comparison ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in normalization_results.items():\n",
    "    plt.plot(range(1, N_EPOCHS_COMPONENT_TEST + 1), history['val_acc'], label=name, marker='.', linestyle='--')\n",
    "\n",
    "plt.title(f'Validation Accuracy Comparison: Normalization Layers ({N_EPOCHS_COMPONENT_TEST} Epochs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'component_normalization_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# == Module Reloading (Optional - Check if needed before optimizer tests) ==\n",
    "# ==============================================================================\n",
    "# print(\"\\n\" + \"=\"*30 + \" Reloading Models Module (Before Optim Tests) \" + \"=\"*30)\n",
    "# if 'models' in sys.modules:\n",
    "#     print(\"Attempting to reload 'models' module...\"); import models; importlib.reload(models); from models import Model_2 # Ensure Model_2 is the latest\n",
    "#     print(\"'models' module reloaded.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))\n",
    "#     BASELINE_MODEL_CLASS = Model_2\n",
    "# else:\n",
    "#     print(\"Importing 'models' module...\"); import models; from models import Model_2\n",
    "#     BASELINE_MODEL_CLASS = Model_2\n",
    "#     print(\"'models' module imported.\"); print(\"Model_2 __init__ signature:\", inspect.signature(Model_2.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# == Optimizer Test ==\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5.2.4 Testing Optimizers ---\")\n",
    "\n",
    "# Define optimizers to test\n",
    "# Note: We may ideally want to re-tune LR slightly for SGD/RMSprop,\n",
    "# but for a direct comparison, we often start with the same LR found for Adam.\n",
    "# --- Redefine optimizers dictionary without lambda for simplicity ---\n",
    "optimizers_to_test = {\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"SGD (momentum=0.9)\": optim.SGD, # Store the SGD class directly\n",
    "    \"RMSprop\": optim.RMSprop,\n",
    "}\n",
    "\n",
    "optimizer_results = {}\n",
    "Optimizer_Test_Model_Class = BASELINE_MODEL_CLASS\n",
    "Optimizer_Activation_Class = OPTIMAL_ACTIVATION_FN\n",
    "Optimizer_Norm_Type = OPTIMAL_NORM_TYPE\n",
    "\n",
    "for optim_name, optim_class in optimizers_to_test.items():\n",
    "    print(f\"\\n-- Testing Optimizer: {optim_name} --\")\n",
    "\n",
    "    model_optim = Optimizer_Test_Model_Class(\n",
    "        activation_fn=Optimizer_Activation_Class,\n",
    "        activation_name=best_act_name,\n",
    "        norm_layer_type=Optimizer_Norm_Type\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion_optim = nn.CrossEntropyLoss()\n",
    "    print(f\"Using config: Activation={best_act_name}, Norm={best_norm_name}, Init=Default\")\n",
    "\n",
    "    # --- Instantiate Optimizer Correctly ---\n",
    "    # Create optimizer instance here, handling SGD parameters specifically\n",
    "    if optim_class == optim.SGD:\n",
    "        optimizer_instance = optim.SGD(model_optim.parameters(), lr=BASELINE_LR, momentum=0.9, weight_decay=BASELINE_WD)\n",
    "        print(f\"Instantiated SGD with momentum=0.9, LR={BASELINE_LR}, WD={BASELINE_WD}\")\n",
    "    else: # For Adam, RMSprop\n",
    "        optimizer_instance = optim_class(model_optim.parameters(), lr=BASELINE_LR, weight_decay=BASELINE_WD)\n",
    "        print(f\"Instantiated {optim_name} with LR={BASELINE_LR}, WD={BASELINE_WD}\")\n",
    "\n",
    "\n",
    "    # --- Modify run_component_training to accept an INSTANCE ---\n",
    "    # (Need to adjust the helper function definition as well)\n",
    "\n",
    "    # --- OR Adjust how we call the helper (Easier) ---\n",
    "    # We will pass the CLASS to the helper, and it will instantiate it.\n",
    "    # BUT, the helper needs modification to handle SGD momentum.\n",
    "    # Let's stick to the original helper and instantiate here, then run manually.\n",
    "\n",
    "    # --- Training Loop (Manual - since helper expects class) ---\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    print(f\"Starting training: {optim_name} - {N_EPOCHS_COMPONENT_TEST} epochs\")\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    for epoch in range(N_EPOCHS_COMPONENT_TEST):\n",
    "        epoch_start_time = time.time()\n",
    "        model_optim.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer_instance.zero_grad() # Use the created instance\n",
    "            outputs = model_optim(inputs)\n",
    "            loss = criterion_optim(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer_instance.step() # Use the created instance\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "\n",
    "        epoch_val_loss, epoch_val_acc = evaluate(model_optim, val_loader, criterion_optim, DEVICE)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        epoch_end_time = time.time()\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or (epoch + 1 == N_EPOCHS_COMPONENT_TEST):\n",
    "            print(f\"  Epoch {epoch+1}/{N_EPOCHS_COMPONENT_TEST} -> Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f} ({(epoch_end_time - epoch_start_time):.2f}s)\")\n",
    "\n",
    "    train_end_time = time.time()\n",
    "    print(f\"Finished training Optim_{optim_name.split(' ')[0]}. Total time: {(train_end_time - train_start_time):.2f}s\")\n",
    "    # --- End Manual Training Loop ---\n",
    "\n",
    "    optimizer_results[optim_name] = history\n",
    "    component_results[f\"Optim_{optim_name.split(' ')[0]}\"] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze Optimizer Results ---\n",
    "print(\"\\n--- Optimizer Results Summary ---\")\n",
    "print(f\"{'Optimizer':<25} | {'Final Val Loss':<15} | {'Final Val Acc':<15} | {'Max Val Acc':<15}\")\n",
    "print(\"-\" * 75)\n",
    "best_optim_name = \"\"\n",
    "best_optim_max_acc = -1.0\n",
    "\n",
    "for name, history in optimizer_results.items():\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    max_val_acc = max(history['val_acc'])\n",
    "    print(f\"{name:<25} | {final_val_loss:<15.4f} | {final_val_acc:<15.4f} | {max_val_acc:<15.4f}\")\n",
    "    if max_val_acc > best_optim_max_acc:\n",
    "        best_optim_max_acc = max_val_acc\n",
    "        best_optim_name = name\n",
    "\n",
    "print(f\"\\nBest Optimizer (based on Max Validation Accuracy): {best_optim_name} ({best_optim_max_acc:.4f})\")\n",
    "# Store the best optimizer class for the final model\n",
    "if best_optim_name in optimizers_to_test:\n",
    "    # Need to handle the lambda case for storing\n",
    "    if isinstance(optimizers_to_test[best_optim_name], type): # Check if it's a class like Adam/RMSprop\n",
    "         OPTIMAL_OPTIMIZER_CLASS = optimizers_to_test[best_optim_name]\n",
    "    else: # It's the SGD lambda\n",
    "         OPTIMAL_OPTIMIZER_CLASS = optim.SGD # Store the base SGD class\n",
    "         print(\"Note: Best optimizer was SGD. Storing base class. Remember to use momentum=0.9.\")\n",
    "    print(f\"Stored OPTIMAL_OPTIMIZER_CLASS: {OPTIMAL_OPTIMIZER_CLASS}\")\n",
    "else:\n",
    "     print(\"Error: Best optimizer name not found!\")\n",
    "     OPTIMAL_OPTIMIZER_CLASS = optim.Adam # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Optimizer comparison ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in optimizer_results.items():\n",
    "    plt.plot(range(1, N_EPOCHS_COMPONENT_TEST + 1), history['val_acc'], label=name, marker='.', linestyle='--')\n",
    "\n",
    "plt.title(f'Validation Accuracy Comparison: Optimizers ({N_EPOCHS_COMPONENT_TEST} Epochs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'component_optimizer_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# == End of Component Optimization ==\n",
    "# ==============================================================================\n",
    "print(\"\\nComponent optimization tests complete.\")\n",
    "print(\"Final selected components based on these tests:\")\n",
    "print(f\" - Model Architecture: {BASELINE_MODEL_CLASS.__name__}\")\n",
    "print(f\" - Learning Rate: {OPTIMAL_LR}\")\n",
    "print(f\" - Weight Decay: {OPTIMAL_WEIGHT_DECAY}\")\n",
    "print(f\" - Initialization: {best_init_name}\") # From previous test summary\n",
    "print(f\" - Activation Function: {best_act_name}\") # From previous test summary\n",
    "print(f\" - Normalization: {best_norm_name}\") # From previous test summary\n",
    "print(f\" - Optimizer: {best_optim_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"--- Phase 6: Final Model Training & Evaluation ---\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Final Configuration ---\n",
    "FINAL_MODEL_CLASS = BASELINE_MODEL_CLASS      # Model_2\n",
    "FINAL_ACTIVATION_FN = OPTIMAL_ACTIVATION_FN # GELU Class\n",
    "FINAL_ACTIVATION_NAME = best_act_name        # 'GELU' String\n",
    "FINAL_NORM_TYPE = OPTIMAL_NORM_TYPE           # None\n",
    "FINAL_OPTIMIZER_CLASS = OPTIMAL_OPTIMIZER_CLASS # RMSprop Class\n",
    "FINAL_LR = OPTIMAL_LR                         # 0.001\n",
    "FINAL_WD = OPTIMAL_WEIGHT_DECAY             # 0.0001\n",
    "# FINAL_INIT = best_init_name # Default - no special function needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for final training - adjust as needed\n",
    "# Monitor validation loss/accuracy to decide when to stop or implement early stopping\n",
    "N_EPOCHS_FINAL = 15\n",
    "\n",
    "print(\"--- Final Model Configuration ---\")\n",
    "print(f\" Model: {FINAL_MODEL_CLASS.__name__}\")\n",
    "print(f\" Activation: {FINAL_ACTIVATION_NAME}\")\n",
    "print(f\" Normalization: {FINAL_NORM_TYPE if FINAL_NORM_TYPE else 'None'}\")\n",
    "print(f\" Optimizer: {FINAL_OPTIMIZER_CLASS.__name__}\")\n",
    "print(f\" Learning Rate: {FINAL_LR}\")\n",
    "print(f\" Weight Decay: {FINAL_WD}\")\n",
    "print(f\" Training Epochs: {N_EPOCHS_FINAL}\")\n",
    "print(\" Initialization: Default\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Final Model ---\n",
    "final_model = FINAL_MODEL_CLASS(\n",
    "    activation_fn=FINAL_ACTIVATION_FN,\n",
    "    activation_name=FINAL_ACTIVATION_NAME,\n",
    "    norm_layer_type=FINAL_NORM_TYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "optimizer_final = FINAL_OPTIMIZER_CLASS(final_model.parameters(), lr=FINAL_LR, weight_decay=FINAL_WD)\n",
    "\n",
    "print(f\"Instantiated Final Model:\\n{final_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Training ---\n",
    "print(\"\\n--- Starting Final Model Training ---\")\n",
    "final_history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "final_training_start_time = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS_FINAL):\n",
    "    epoch_start_time = time.time()\n",
    "    # Training phase\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader: # Train on original training set\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer_final.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion_final(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    final_history['train_loss'].append(epoch_train_loss)\n",
    "\n",
    "    # Validation phase (Monitor on validation set)\n",
    "    epoch_val_loss, epoch_val_acc = evaluate(final_model, val_loader, criterion_final, DEVICE)\n",
    "    final_history['val_loss'].append(epoch_val_loss)\n",
    "    final_history['val_acc'].append(epoch_val_acc)\n",
    "    epoch_end_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS_FINAL} -> Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f} ({(epoch_end_time - epoch_start_time):.2f}s)\")\n",
    "\n",
    "    # Basic Early Stopping Check (Example - can be made more robust)\n",
    "    # Stop if validation loss hasn't improved for N epochs (e.g., patience=5)\n",
    "    patience = 5\n",
    "    if epoch >= patience:\n",
    "        # Check if current val_loss is worse than loss 'patience' epochs ago\n",
    "        if epoch_val_loss > min(final_history['val_loss'][-(patience+1):-1]):\n",
    "             print(f\"Validation loss has not improved for {patience} epochs. Consider stopping early.\")\n",
    "             # break # Uncomment to actually stop training\n",
    "\n",
    "final_training_end_time = time.time()\n",
    "print(f\"\\nFinished final training. Total time: {(final_training_end_time - final_training_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Final Training History ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1); plt.plot(range(1, len(final_history['train_loss']) + 1), final_history['train_loss'], label='Training Loss'); plt.plot(range(1, len(final_history['val_loss']) + 1), final_history['val_loss'], label='Validation Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Final Model Training Loss'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2); plt.plot(range(1, len(final_history['val_acc']) + 1), final_history['val_acc'], label='Validation Accuracy', color='green'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Final Model Validation Accuracy'); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout(); plt.savefig(os.path.join(PLOT_SAVE_DIR, 'final_model_training_history.png')); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating Final Model on Test Set ---\")\n",
    "test_loss, test_accuracy = evaluate(final_model, test_loader, criterion_final, DEVICE)\n",
    "print(f\"\\nPerformance on the HELD-OUT TEST SET:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Save the Final Model ---\n",
    "model_save_path = os.path.join(\"../results/models/\", \"final_optimized_model.pth\")\n",
    "print(f\"\\nSaving final model state_dict to {model_save_path}\")\n",
    "torch.save(final_model.state_dict(), model_save_path)\n",
    "print(\"Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
