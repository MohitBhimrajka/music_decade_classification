{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Path to the raw data file\n",
    "RAW_DATA_PATH = \"../data/raw/YearPredictionMSD.txt\"\n",
    "# Directory to save generated plots\n",
    "PLOT_SAVE_DIR = \"../results/plots/\"\n",
    "# Ensure plot directory exists\n",
    "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Optional: Add src to path to reuse functions ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_processing import load_data, create_decade_bins\n",
    "#If not reusing functions from src, redefine them or necessary parts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "print(f\"Loading raw data from: {RAW_DATA_PATH}\")\n",
    "try:\n",
    "    # Define column names as in data_processing.py\n",
    "    N_FEATURES = 90\n",
    "    colnames = ['Year'] + [f'Feature_{i+1}' for i in range(N_FEATURES)]\n",
    "    df_raw = pd.read_csv(RAW_DATA_PATH, header=None, names=colnames)\n",
    "    print(f\"Data loaded successfully. Shape: {df_raw.shape}\")\n",
    "    print(\"\\nFirst 5 rows of raw data:\")\n",
    "    print(df_raw.head())\n",
    "    print(\"\\nBasic data info:\")\n",
    "    df_raw.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Raw data file not found at {RAW_DATA_PATH}. Please ensure it's downloaded.\")\n",
    "    # Exit or handle error appropriately in a real script\n",
    "    # For a notebook, we might just stop execution here or raise the error\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decade Binning (Reproduce logic from data_processing.py) ---\n",
    "print(\"\\nCreating decade bins for analysis...\")\n",
    "min_year = 1920 # Start decade reference\n",
    "df_raw['Decade_Start'] = (df_raw['Year'] // 10) * 10\n",
    "df_raw['Decade_Label'] = ((df_raw['Decade_Start'] - min_year) // 10).astype(int)\n",
    "df_raw['Decade_Label'] = df_raw['Decade_Label'].clip(lower=0) # Clip years < 1920\n",
    "decade_map = {i: f\"{min_year + i*10}s\" for i in range(10)}\n",
    "df_raw['Decade_Name'] = df_raw['Decade_Label'].map(decade_map)\n",
    "print(\"Decade columns ('Decade_Label', 'Decade_Name') added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Class Balance Analysis ---\n",
    "print(\"\\n--- 1. Class Balance Analysis ---\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_raw, x='Decade_Name', order=[decade_map[i] for i in range(10)], palette='viridis')\n",
    "plt.title('Distribution of Songs Across Decades')\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'eda_decade_distribution.png'))\n",
    "plt.show()\n",
    "\n",
    "decade_counts = df_raw['Decade_Name'].value_counts().sort_index()\n",
    "print(\"\\nSong Counts per Decade:\")\n",
    "print(decade_counts)\n",
    "print(f\"\\nObservations: The dataset is heavily imbalanced, with a vast majority of songs from the 2000s, followed by the 1990s. Earlier decades have significantly fewer samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Feature Distribution Analysis ---\n",
    "print(\"\\n--- 2. Feature Distribution Analysis ---\")\n",
    "# Select a subset of features for detailed analysis (e.g., first 12, often timbre averages)\n",
    "# and maybe a few from the covariance features later on.\n",
    "features_to_plot = df_raw.columns[1:13] # Features 1 to 12 (Timbre Averages)\n",
    "print(f\"Plotting distributions for features: {list(features_to_plot)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(features_to_plot):\n",
    "    plt.subplot(3, 4, i + 1) # Adjust grid size (3x4) as needed\n",
    "    sns.histplot(df_raw[col], kde=True, bins=50)\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.suptitle('Distribution of First 12 Features (Timbre Averages)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'eda_feature_distributions_hist.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots can also show distribution and outliers\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=df_raw[features_to_plot], palette='viridis')\n",
    "plt.title('Box Plots of First 12 Features (Timbre Averages)')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'eda_feature_distributions_box.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nObservations: Examine the plots for skewness, modality (number of peaks), and spread. Many features might appear roughly normally distributed but could have long tails (indicating outliers).\")\n",
    "print(\"Numerical summary:\")\n",
    "print(df_raw[features_to_plot].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Correlation Analysis ---\n",
    "print(\"\\n--- 3. Correlation Analysis ---\")\n",
    "# Calculate correlation matrix for numerical features (excluding Year and derived decade cols)\n",
    "feature_cols = [col for col in df_raw.columns if col.startswith('Feature_')]\n",
    "correlation_matrix = df_raw[feature_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt=\".1f\", linewidths=.5) # annot=True is too crowded for 90 features\n",
    "plt.title('Correlation Matrix of Audio Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, 'eda_correlation_heatmap.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated pairs (optional)\n",
    "threshold = 0.8\n",
    "# Create a mask for the upper triangle (including diagonal)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "# Apply mask and find correlations above threshold\n",
    "highly_correlated_filtered = correlation_matrix.mask(mask)\n",
    "highly_correlated_filtered = highly_correlated_filtered[abs(highly_correlated_filtered) > threshold]\n",
    "\n",
    "corr_pairs = highly_correlated_filtered.unstack().dropna().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nHighly Correlated Feature Pairs (Threshold > {threshold}):\")\n",
    "if not corr_pairs.empty:\n",
    "    print(corr_pairs)\n",
    "else:\n",
    "    print(f\"No feature pairs found with absolute correlation above the threshold {threshold}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Outlier Analysis (using Box Plots from Feature Distribution) ---\n",
    "print(\"\\n--- 4. Outlier Analysis ---\")\n",
    "print(\"Refer back to the box plots generated in the 'Feature Distribution Analysis' section.\")\n",
    "print(\"Box plots visually indicate potential outliers as points beyond the 'whiskers'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate IQR bounds for one feature\n",
    "feature_example = 'Feature_1'\n",
    "Q1 = df_raw[feature_example].quantile(0.25)\n",
    "Q3 = df_raw[feature_example].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_raw[(df_raw[feature_example] < lower_bound) | (df_raw[feature_example] > upper_bound)]\n",
    "print(f\"\\nExample Outlier Check for '{feature_example}':\")\n",
    "print(f\"  IQR: {IQR:.2f}\")\n",
    "print(f\"  Lower Bound (Q1 - 1.5*IQR): {lower_bound:.2f}\")\n",
    "print(f\"  Upper Bound (Q3 + 1.5*IQR): {upper_bound:.2f}\")\n",
    "print(f\"  Number of potential outliers (based on 1.5*IQR rule): {len(outliers)}\")\n",
    "print(f\"  Percentage of potential outliers: {len(outliers) / len(df_raw) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nObservations & Handling Strategy:\")\n",
    "print(\" - Many features show points beyond the 1.5*IQR whiskers, suggesting the presence of outliers.\")\n",
    "print(\" - Strategy Decision: For this project, we used StandardScaler in data_processing.py. While StandardScaler is sensitive to outliers, deep learning models (especially with techniques like Batch Norm, which we might test later) can sometimes be relatively robust.\")\n",
    "print(\" - Alternative strategies (not implemented here but considered):\")\n",
    "print(\"   - Use RobustScaler: Scales using percentiles, less sensitive to outliers.\")\n",
    "print(\"   - Clipping: Cap feature values at certain percentiles (e.g., 1st and 99th).\")\n",
    "print(\"   - Transformation: Apply log or Box-Cox transforms if features are highly skewed.\")\n",
    "print(\" - Chosen Approach: Proceed with StandardScaler, acknowledging the presence of outliers. We will monitor model performance and may revisit outlier handling if necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Categorical Features ---\n",
    "print(\"\\n--- 5. Categorical Features ---\")\n",
    "# Check data types again after loading\n",
    "print(df_raw.info())\n",
    "# Identify non-numeric columns (excluding our derived Decade_Name)\n",
    "categorical_cols = df_raw.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"\\nPotential categorical columns detected (excluding Decade_Name): {list(categorical_cols.drop('Decade_Name', errors='ignore'))}\")\n",
    "print(\"Observations: As expected for this dataset, all original predictor columns (Feature_1 to Feature_90) are numeric (float64). No categorical feature embedding strategy is required for the predictors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary of EDA Findings ---\n",
    "print(\"\\n--- Summary of Key EDA Findings ---\")\n",
    "print(\"1.  **Target Variable (Decade):** Heavily imbalanced, dominated by 2000s and 1990s.\")\n",
    "print(\"2.  **Features:** All 90 predictor features are numeric (float).\")\n",
    "print(\"3.  **Distributions:** Feature distributions vary. Some are roughly normal, others might be skewed or have multiple peaks (visual inspection needed per feature).\")\n",
    "print(\"4.  **Correlations:** Some correlations exist between features, particularly noted visually within blocks (e.g., early timbre features, later covariance features). No extremely high correlations (>0.95) jumped out immediately in the sample check, but moderate correlations are present.\")\n",
    "print(\"5.  **Outliers:** Potential outliers detected in many features based on visual inspection of box plots and IQR checks.\")\n",
    "print(\"6.  **Missing Values:** No missing values detected by `df.info()` (consistent with dataset description).\")\n",
    "print(\"7.  **Preprocessing Decisions (Recap):**\")\n",
    "print(\"    - Decade binning successfully converted regression to classification.\")\n",
    "print(\"    - Stratified splitting addressed the class imbalance during data partitioning.\")\n",
    "print(\"    - StandardScaler was used for feature scaling, acknowledging outlier presence.\")\n",
    "print(\"    - No categorical encoding needed for predictors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
