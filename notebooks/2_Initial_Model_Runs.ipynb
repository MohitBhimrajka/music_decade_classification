{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import copy  # To store model state if needed\n",
    "\n",
    "# Ensure the src directory is in the Python path\n",
    "# Adjust the path '..' if your notebook is in a different location relative to src\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import modules from src\n",
    "from utils import load_processed_data\n",
    "from models import Model_1, Model_2, Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 128 # Reasonable batch size (can try 64, 256)\n",
    "LEARNING_RATE = 1e-3 # A common default starting LR for Adam\n",
    "N_MINIBATCHES = 15\n",
    "EVAL_INTERVAL = 5 # Evaluate on validation set every X mini-batches\n",
    "SEED = 42 # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Seed ---\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# Note: MPS backend reproducibility might have limitations, but setting CPU/CUDA seeds is good practice.\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Reproducibility seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, _, _ = load_processed_data()\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# Use shuffle=True for training to ensure batches are different each epoch\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "# No need to shuffle validation data\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2) # Larger batch size for faster validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Training and Evaluation Functions ---\n",
    "\n",
    "def train_one_step(model, batch, criterion, optimizer, device):\n",
    "    \"\"\"Performs a single training step (forward pass, loss calc, backward pass, optimizer step).\"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    inputs, targets = batch\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on the given data loader.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "        for batch in loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0) # Accumulate loss weighted by batch size\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Setup ---\n",
    "model_architectures = {\n",
    "    \"Model_1 (128x128)\": Model_1,\n",
    "    \"Model_2 (256x256)\": Model_2,\n",
    "    \"Model_3 (256x128x64)\": Model_3\n",
    "}\n",
    "\n",
    "results = {} # To store detailed results\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Experiments ---\n",
    "\n",
    "for name, ModelClass in model_architectures.items():\n",
    "    print(f\"\\n--- Running Experiment for: {name} ---\")\n",
    "    # Re-seed generator for dataloader for each model if desired (optional, but good practice)\n",
    "    # train_loader.generator.manual_seed(SEED) # Reset iterator state implicitly\n",
    "    train_iter = iter(train_loader) # Create a fresh iterator\n",
    "\n",
    "    # Instantiate model and move to device\n",
    "    # Note: Parameter initialization depends on the global torch seed set earlier\n",
    "    model = ModelClass().to(DEVICE)\n",
    "    print(model) # Print architecture details\n",
    "\n",
    "    # Use Adam optimizer with a default learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Store performance metrics\n",
    "    minibatch_losses = []\n",
    "    eval_batches = [] # Batch numbers where evaluation was performed\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop for N_MINIBATCHES\n",
    "    batch_count = 0\n",
    "\n",
    "    while batch_count < N_MINIBATCHES:\n",
    "        try:\n",
    "            # Fetch the next batch\n",
    "            batch = next(train_iter)\n",
    "        except StopIteration:\n",
    "            # Reset iterator if it runs out (shouldn't happen in 15 batches normally)\n",
    "            print(\"Resetting train_loader iterator...\")\n",
    "            train_iter = iter(train_loader)\n",
    "            batch = next(train_iter)\n",
    "\n",
    "        # Perform one training step\n",
    "        loss = train_one_step(model, batch, criterion, optimizer, DEVICE)\n",
    "        minibatch_losses.append(loss)\n",
    "        batch_count += 1\n",
    "\n",
    "        # Optional: Print progress\n",
    "        # print(f\"  Batch {batch_count}/{N_MINIBATCHES}, Loss: {loss:.4f}\") # Can be verbose\n",
    "\n",
    "        # Intermediate Evaluation\n",
    "        if batch_count % EVAL_INTERVAL == 0 or batch_count == N_MINIBATCHES:\n",
    "            eval_start_time = time.time()\n",
    "            val_loss, val_accuracy = evaluate(model, val_loader, criterion, DEVICE)\n",
    "            eval_end_time = time.time()\n",
    "            eval_batches.append(batch_count)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            print(f\"  Batch {batch_count}/{N_MINIBATCHES} -> Train Loss (last batch): {loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f} (Eval took {eval_end_time - eval_start_time:.2f}s)\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "\n",
    "    print(f\"Finished {name}.\")\n",
    "    print(f\"  Total Training Time for {N_MINIBATCHES} batches: {total_training_time:.2f} seconds\")\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'minibatch_losses': minibatch_losses,\n",
    "        'eval_batches': eval_batches, # e.g., [5, 10, 15]\n",
    "        'val_losses': val_losses,     # List of val losses at eval points\n",
    "        'val_accuracies': val_accuracies, # List of val accuracies at eval points\n",
    "        'final_val_loss': val_losses[-1], # Get the last recorded val loss\n",
    "        'final_val_accuracy': val_accuracies[-1], # Get the last recorded val accuracy\n",
    "        'training_time': total_training_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss (per mini-batch)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 1, 1) # Create subplot 1\n",
    "for name, data in results.items():\n",
    "    plt.plot(range(1, N_MINIBATCHES + 1), data['minibatch_losses'], label=f\"{name}\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Mini-batch Number\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss per Mini-batch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, N_MINIBATCHES + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Validation Performance (at evaluation intervals)\n",
    "plt.subplot(2, 1, 2) # Create subplot 2\n",
    "for name, data in results.items():\n",
    "    # Plot validation accuracy\n",
    "    plt.plot(data['eval_batches'], data['val_accuracies'], label=f\"{name} Val Acc\", marker='o', linestyle='-')\n",
    "    # Optionally plot validation loss on a secondary y-axis if scales differ too much\n",
    "    # (Let's keep it simple for now and focus on accuracy)\n",
    "\n",
    "plt.xlabel(\"Mini-batch Number (Evaluation Points)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(f\"Validation Accuracy at Evaluation Intervals (every {EVAL_INTERVAL} batches)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(results[list(results.keys())[0]]['eval_batches']) # Use eval batches from first result as ticks\n",
    "\n",
    "plt.tight_layout() # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table (using final validation metrics after 15 batches)\n",
    "print(\"\\n--- Summary of Initial Runs (Performance after 15 Mini-batches) ---\")\n",
    "print(f\"{'Architecture':<25} | {'Final Val Loss':<15} | {'Final Val Accuracy':<18} | {'Training Time (s)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for name, data in results.items():\n",
    "    print(f\"{name:<25} | {data['final_val_loss']:.4f}{' ':<10} | {data['final_val_accuracy']:.4f}{' ':<13} | {data['training_time']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select Best Performing Architecture ---\n",
    "# Based on final validation accuracy primarily\n",
    "best_model_name = \"\"\n",
    "best_val_accuracy = -1.0\n",
    "\n",
    "for name, data in results.items():\n",
    "    if data['final_val_accuracy'] > best_val_accuracy:\n",
    "        best_val_accuracy = data['final_val_accuracy']\n",
    "        best_model_name = name\n",
    "    # Could add tie-breaking logic using final_val_loss if needed\n",
    "\n",
    "print(f\"\\nBased on final validation accuracy after {N_MINIBATCHES} batches, the best performing architecture appears to be: {best_model_name}\")\n",
    "print(f\"(Achieved {best_val_accuracy:.4f} accuracy)\")\n",
    "print(\"(Note: This is based on very limited training. The validation trajectory plot provides more context.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
